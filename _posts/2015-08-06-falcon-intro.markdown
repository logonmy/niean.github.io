---
layout: post
title: 小米监控系统Falcon简析
date: 2015-08-06 20:00
tags:
  - monitor
---

加入小米监控团队有半年时间了，能与领域里的牛人们一起共事，很开心。当然，时不时会想起巴巴am团队的童鞋，多少有些感怀。闲话不多说了，今天介绍下小米的监控产品[Falcon](https://github.com/open-falcon)。

Falcon是一个企业级的监控解决方案，偏向于平台化，主要提供 实时报警、监控数据展示等功能。本文从系统架构、数据模型、数据收集、数据存储、图表展示、实时报警等方面，介绍Falcon的实现。本文的部分章节，参考了开源版本Open-Falcon的[GitBook](http://book.open-falcon.com/zh/intro/README.html)。

<br />
## 系统架构
**服务模块**
![archi](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/arch.png)

转发服务Transfer，负责收集监控数据，并将收集到的监控数据分片、发送给Judge&Graph（以及正在测试中的 持久化存储rrd.hbase）。

Graph组件收到监控数据后，将这些数据悉数存储在本地的rrd数据库，并借助rrd的归档功能将这些监控数据进行抽样存储。Query组件提供查询绘图数据的Api入口，从Graph组件上读取数据、返回给高层的监控数据消费者。Dashboard是展示监控数据的WEB应用，从Query拿数据、展示在UI上。

Judge组件负责报警判断，它从Transfer处获取监控数据、从HBS处获取报警策略的配置，进行实时的报警判断，生成报警事件。Alarm组件消费Judge产生的报警事件，并产生有效的报警动作。当前，Falcon支持短信、邮件、Http回调等报警动作。

Portal是Falcon的门户，为用户提供了配置报警策略、集群监控、Nodata的UI，并将这些配置信息存储至mysql。HBS是Falcon的配置中心，消费mysql的配置信息、并转化为后端服务乐于接受的形式。

集群监控Aggr和Nodata监控，是Falcon近期新增的服务组件。Aggr将多个单机的监控信息，聚合成集群维度，然后吐回监控系统。Nodata则实时监督监控数据的上报，发现中断的情况就产生用户自定义的超时数据、并吐回到监控系统。


**部署方式**
![deploy](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/deploy.png)

Falcon是一个比较大的分布式系统，有十几个组件。按照功能，这十几个组件可以划分为 基础组件、作图链路组件和报警链路组件，安装部署的架构如下图所示。其中，基础组件以绿色标注圈住、作图链路组件以蓝色圈住、报警链路组件以红色圈住，橙色填充的组件为域名。

这里不展开讲了。如果需要了解小米公司部署Falcon的详细经验，请参考[《Falcon部署实践》](http://book.open-falcon.com/zh/deploy/README.html)一文。

<br />
## 数据采集
在介绍"服务模块"时，我可以忽略了`Falcon-Agent`这一组件。`Falcon-Agent`是监控系统提供的、用于采集监控数据的组件，顺便也做一点Proxy的工作。那么，监控系统是否有必要***亲力亲为地负责监控数据采集***？

我认为，监控系统的核心工作是实时报警和数据存储，把这两个核心工作搞定了，监控系统就能健康的run起来。只要数据模型抽象的好，实时报警 和 数据存储 就能够做的很通用、很纯粹、很"技术"，进而很容易维护(监控系统平台化，这么看来是可以实现的)。

反观监控数据采集，经常会涉及到较多、较深的业务细节。比如，网络设备的状态数据采集，会因为生产厂商、设备型号、设备类型的差异，导致状态数据的采集会有很大差异。再比如，线上业务数据，这更是千差万别、几乎不可能 依靠监控团队的一己之力 来搞定这件事。过多的涉及业务细节，必然会被业务所累，需要花费大精力了解业务、需要时刻应对业务变更带带来的影响、甚至需要面对一些业务团队的经常性的"骚扰"——一不小心，监控开发团队的主业就发生变化了，呵呵。

当然，有很多数据采集需求是通用的、或者较通用的，如果监控系统能够帮忙做数据采集，会大大提高全局的工作效率。第一个能想到的，就是服务器信息。针对这个通用需求，我们开发了机器端代理Falcon-Agent，Agent会主动采集服务器上的状态信息，比如 CPU、Load、Mem、IO、Net等等，用户不需要做任何工作，就能获得这些监控数据(前提是,系统预装Falcon-Agent)。另外，有一些数据采集需求跟服务器紧密相关、但涉及的机器范围有限、不能通过Agent通用采集来完成。如果让用户自己去采集这些指标数据，又挺麻烦。于是我们在Agent之上提供了[插件机制](http://book.open-falcon.com/zh/philosophy/plugin.html)，用户可以按照需要编写插件脚本、指定哪些机器执行这些脚本，然后Agent就会按照用户的意愿去指定的机器上执行指定的插件。插件机制，本质上是Agent通用采集功能的一个扩展。

唠叨了半天，结论是什么呢？**监控只负责最通用的数据采集，非通用的情况需要由用户自己完成数据采集工作**。对于非通用情况，监控只需要制定准入规范、准备数据收集器，让用户按照标准把数据push到数据收集器即可。

TODO: Java业务监控、Nginx监控、Mysql监控等的介绍，这些不应该被详细描述、大概提提就行。

<br />
## 数据模型
数据模型，就是监控系统的数据准入规范。

数据模型，要包含完整的信息。Falcon不要求用户预定义采集指标，因此数据模型需要包含完整的"身份信息"，通过一条监控数据就能确定该采集项的各种元信息。

数据模型，要足够灵活。比如以zabbix为例，上报的数据为hostname、metric，那么用户添加告警策略、管理告警策略的时候，就只能以这两个维度进行。举一个最常见的场景：

hostA的磁盘空间，小于5%，就告警。一般的服务器上，都会有两个主要的分区，根分区和home分区，在zabbix里面，就得加两条规则；如果是hadoop的机器，一般还会有十几块的数据盘，还得再加10多条规则，这样就会痛苦，不幸福，不利于自动化。

借鉴OpenTSDB的数据结构，结合自己的业务特点，得到如下数据模型:

```
	{
	    metric: load.1min,
	    endpoint: c3-op-mon-graph01.bj,
	    tags: srv=falcon,idc=c3,module=graph,
	    value: 1.5,
	    timestamp: `date +%s`,
	    counterType: GAUGE,
	    step: 60
	},
	{
	    metric: net.port.listen,
	    endpoint: c3-op-mon-graph01.bj,
	    tags: port=3306,
	    value: 1,
	    timestamp: `date +%s`,
	    counterType: GAUGE,
	    step: 60
	}
```
通过 endpoint+metric+tags, 可以唯一确定一个采集指标; 通过 step+counterType, 可以获取该采集指标的周期、数据类型。

通过 endpoint/metric/tags 这种数据结构, 可以灵活的配置报警、配置绘图展示等。

<br />
## 数据收集
从用户角度，Falcon提供了两个数据收集器:Agent和Transfer。Agent最终会把收集到的数据上报给Transfer。

Agent收集器，提供本机数据收集的通道。Agent被安装在每一台服务器上、覆盖面很广，用户使用起来很方便。如，用户通过linux的crontab来采集监控数据，采集完成后直接push到本地的Agent上即可。向本地Agent上报数据，一般不会有ACL的问题。监控数据的量很大，不宜使用本地Agent上报，因为Agent的数据路由能力有限、且容易消耗过多的本机资源。

Transfer收集器，提供了远端数据收集通道。对于数据量很大的采集服务，适合直接将数据push给Transfer。Transfer的单机数据接收能力很强，且集群部署，整体上可以hold住用户的大流量push。如，负责用户访问质量分析的Storm集群，产生海量的监控数据，这些数据是通过Transfer上报至监控系统的。Transfer一般部署在中心机房，向其push数据时会有ACL的问题，需要注意。

多IDC时，可能面对 "分区到中心的专线网络质量较差&公网ACL不通" 等问题。这时，可以在分区内部署一套数据路由服务，接收本分区内的所有流量，然后通过公网，将数据push给中心的Transfer。如下图，
![gateway](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/gateway.png)

<br />
## 数据存储
一般来说，监控系统收集的数据量很大，写多读少，要求历史数据的查询要快速、高效。

在Falcon中，Graph组件负责绘图数据的存储。根据监控数据的特点，Graph在存储绘图数据时使用了Rrdtool。关于Rrdtool的概念，请参考其[官网](http://oss.oetiker.ch/rrdtool/doc/index.en.html)。

Graph组件从Transfer接收分片数据，然后将数据写入本地的Rrdtool。在数据每次存入的时候，Rrdtool会自动进行采样、归档，策略如下。历史数据保存5年。同时为了不丢失信息量，数据归档的时候，会按照平均值采样、最大值采样、最小值采样存三份。以一分钟的上报周期为例子，下面是我们采用的归档策略:<br />

```
	// 1分钟一个点存 12小时
	c.RRA("AVERAGE", 0.5, 1, 720)

	// 5m一个点存2d
	c.RRA("AVERAGE", 0.5, 5, 576)
	c.RRA("MAX", 0.5, 5, 576)
	c.RRA("MIN", 0.5, 5, 576)

	// 20m一个点存7d
	c.RRA("AVERAGE", 0.5, 20, 504)
	c.RRA("MAX", 0.5, 20, 504)
	c.RRA("MIN", 0.5, 20, 504)

	// 3小时一个点存3个月
	c.RRA("AVERAGE", 0.5, 180, 766)
	c.RRA("MAX", 0.5, 180, 766)
	c.RRA("MIN", 0.5, 180, 766)

	// 1天一个点存5year
	c.RRA("AVERAGE", 0.5, 720, 730)
	c.RRA("MAX", 0.5, 720, 730)
	c.RRA("MIN", 0.5, 720, 730)
```

用户查询某个counter时，Rrd会根据查询的时间段，选择合适的归档数据。比如，12小时以内的查询返回原始数据点，12小时至2天的 返回5m采样的点，1年的时间跨度返回1d采样的点。 Rrdtool的采样及查询策略，很好的保障了绘图数据读取的速率。

TODO: 为了搞定大量写入，Graph做了哪些工作。

<br />
## 数据查询
Query组件，对用户提供数据查询服务，接口形式为Http-Post。通过Query的接口，用户可以查询指定时间段内、一批监控指标的绘图数据。接口定义，见[这里](http://book.open-falcon.com/zh/usage/query.html)。

为什么要提供数据查询的专门组件呢？有两个原因。

+ 监控数据按照哈希规则，分片存储在多个Graph实例上。Query提供一个统一查询入口、对外屏蔽数据分片的规则，用户查询数据时会变得更方便、监控数据的存储也会更安全。
+ 数据和用户操作之间，隔上一层Api，这是很常规的做法。通过这个Api层，可以方便的实现权限控制、流量控制等。


<br />
## 图表展示

展示监控数据的历史曲线，这是监控系统两大核心功能之一。Falcon提供了两种图表展示的方式: 动态查询 和 固定展示。动态查询，用于满足用户的临时查询需求。固定展示，为用户提供了定制绘图曲线的入口。

**动态查询**

排查问题的过程中，用户经常需要临时性的查询某一个指标项的历史曲线；问题搞定后，用户可能就不在关注这个指标项了。这种情况下，用户可以进行动态查询，操作步骤为: 输入 endpoint 或 tag对儿，筛选出所有符合条件的endpoints; 再选中关注的endpoints，结合输入的metric/tags，查询出所有满足条件的counter; 选中endpoints + counters，就可以查看绘图曲线。

动态查询页面
![动态查询](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/dashboard.1.png)

动态查询的绘图曲线页面
![动态查询](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/dashboard.2.png)


**固定展示**

有一些监控指标，需要用户**长期**关注。这些指标，往往是**确定**的、**有限**的。这种情况下，用户可以使用Falcon提供的固定展示功能，操作步骤为: 定制Screen图表，一个Screen里面可以包含多个图表; 定制完成后，访问该Screen、查看图表。两步搞定，高度个性化，就这么流弊，呵呵。

配置Screen
![动态查询](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/screen.1.png)

查看Screen
![动态查询](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/screen.2.png)

整理一句，将绘图曲线展示和监控实体管理分开，很赞。这种逆天的设计，带来了很多灵活性，但需要在监控推广的早期逐渐培养用户的使用习惯、使之接受这种设计。看看大公司里的各种各样积重难返、步履维艰，深深觉着在小公司里从零搞起一个项目也是很幸福滴；同时由衷的认为，用户习惯的培养真真太重要了。


<br />
## 报警配置
监控报警的三要素，为: 监控指标的数据，判断条件，报警。报警动作、报警抑制、报警信息接收人等，均包含在"报警"这一要素中，不详细展开。这里认为，判断条件+报警=报警策略。

报警配置，可以概括为: 某个监控指标的数据，应用，某个报警策略。用表达式表示出来，如下:

```
	each(metric=$Metric, tag1=$Tag1, tag2=$Tag2, ...)
	do strategy(){
		if ( $Value -opt $Threshold ){
			alarm($Action, $Team){
				if (rules()) {
					do $Action to $Team
				}
			}
		}
	}
```

引入机器分组和策略模板的概念，配置表达式进化为:

```
	// 普适表达式
	each( metric=$M[,endpoint=$E, tag1=$T1, ...] ) 
	do strategy(){...}

	// 常用策略，基于机器分组和策略模板
	each( metric=$M, hostgrp=$HostGrp[, tag1=$T1, ...] )
	template2() extend template1 { // 继承
		do strategy1-1(){...} // 覆盖
		do strategy2-1(){...} // 新增
		...
	}
```

Falcon报警配置，支持普适表达式，也支持"基于机器分组和策略模板(简称为常用策略)"的配置。

**普适表达式**

这是普适表达式的配置页面。一般的，endpoint不是机器名称时，使用普适表达式；endpoint为机器名时，建议使用常用策略。
![hostgrp](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/expression.1.png)


**常用策略**

这是机器分组的管理页面，某个endpoint加入到某个机器分组中，就会自动拥有该分组所绑定的所有策略列表。此处可以和服务树结合，服务器进出服务树节点，相关的模板会自动关联或者解除。这样服务上下线，都不需要手动来变更监控，大大提高效率，降低遗漏和误报警。
![hostgrp](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/strategy.1.png)

一个最简单的模板的例子，模板支持继承和覆盖，模板和机器分组绑定后，该机器分组下的机器会自动应用该模板的所有策略。
![hostgrp](https://raw.githubusercontent.com/niean/niean.github.io/master/images/20150806/strategy.2.png)


**报警配置演进**

报警配置 的优化过程，就是 不断优化监控数据的筛选、不断优化报警策略配置的过程。

> 1.普适形式 

```
	each( metric=$M[,endpoint=$E, tag1=$T1, ...] ) 
	do strategy(){...}
```
一个监控指标的数据，对应一个报警策略。有比这更通用的吗？不可能有! 有比这更难用的吗？估计也没有，因为大部分时候 用户都需要很多个配置才能搞定自己的需求。


> 2.分组管理机器

很多时候，监控实体都是机器、endpoint是机器名。这种情况下，endpoint是筛选监控数据的一个主要且通用的维度。

按机器名筛选监控数据，最直接的形式为:

```
	each( metric=$M, endpoint=$HostName[, tag1=$T1, ...] )
	do strategy(){...}
```

为每个机器的指标项 进行报警配置，机器一多，配置就会很麻烦，配置的维护也需要很多人力。 很自然的，我们想对机器归归类、许多特征一致的机器被放在一个分组进行集中管理，于是抽象出机器分组hostgrp的概念；配置报警时，用机器分组代替机器，监控后台再把报警策略展开到分组内的机器上(机器和机器分组的关系需要在某个地方去维护)。这样，报警配置进化为:

```
	each( metric=$M, hostgrp=$HostGrp[, tag1=$T1, ...] )
	do strategy(){...}
```

以机器分组维度，代替机器维度，能够大大减少报警配置的数据。但，要保留下普适方式，来覆盖小众(endpoint不是机器名的情况)的需求。另外，使用hostgrp还带来了一个意外的好处: 报警配置动态生效，机器加入某个分组、则能够获得此分组上的报警策略，机器移出某个分组、通过原分组获取的报警策略也将被移除。 


> 3.策略模板

一个报警配置包含一条策略，需要几个策略就要几个配置，即使此时的监控数据是一样的，有点冗余、配置起来麻烦多。为了提高配置效率，把相关的策略绑在一起、构成一个策略组、形成一个新的概念: 策略模板。有个策略模板，配置形态也就发生了变化，原来是监控数据对应一条策略，现在可以对应一组策略了，如下:

```
	each( metric=$M, hostgrp=$HostGrp[, tag1=$T1, ...] )
	template(){
		do strategy1(){...}
		do strategy2(){...}
		...
	}
```

上面的配置形式，已经很简化。如果策略模板能够被复用，将进一步简化 策略模板的配置。复用，以继承的方式实现；有继承，就有覆盖，来满足"策略模板中的部分表达式需要特殊化"这个需求。生生的，又迈出了一个大步子，现在配置形式如下:

```
	each( metric=$M, hostgrp=$HostGrp[, tag1=$T1, ...] )
	template2() extend template1 { // 继承
		do strategy1-1(){...} // 覆盖
		do strategy2-1(){...} // 新增
		...
	}
```

打完收工。报警配置的一个终态如下，既方便使用、又照顾小众需求。

```
	// 普适表达式
	each( metric=$M[,endpoint=$E, tag1=$T1, ...] ) 
	do strategy(){...}

	// 机器相关的报警策略
	each( metric=$M, hostgrp=$HostGrp[, tag1=$T1, ...] )
	template2() extend template1 { // 继承
		do strategy1-1(){...} // 覆盖
		do strategy2-1(){...} // 新增
		...
	}
```

这里有几点，需要特别指出:

+ 继承&覆盖这两个特性，只发生在策略模板上。监控系统的机器分组，不存在所谓的继承&覆盖特性
+ 监控系统的机器分组，是一级的扁平结构，机器分组之间无继承关系。很多公司，机器管理都是借助"服务树"进行的。在设计监控系统时，他们把服务树当成稳定的、可依赖的基础设施，因此，把服务树节点当做机器分组来使用、通过服务树节点的上下级关系实现继承和覆盖。而实践中，服务树是可能发生剧烈变化的，甚至会有多棵树的出现，这时监控系统就只能死翘翘了。一种可行的改造方案是: 将服务树各节点扁平化为一级节点，去掉节点之间的上下级关系、只保留其机器分组管理的特征；转而，将继承和覆盖的属性赋予策略模板。


<br />
## 结束语
总的来说，Falcon在理念、实现上都是非常先进的，领先传统厂商至少一代。主要体现在以下几点:

+  数据收集，不需要预定义、提供了完善的数据收集器，方便用户自己push数据。从而，监控可以做到不过分关注数据采集
+  绘图展示，灵活而又能定制
+  实时报警，配置高效而灵活，使用tag过滤机制、使用hostgrp管理实体、使用template管理策略
+  机器分组，设计成一级扁平结构，使得监控系统可以适应CMDB的结构变更
+  其他方面，纯粹的平台化理念、成功的用户习惯培养、良好的工程控制，等等

不多说了，哥哥们又来催上线了。最近搞了几个故障，快被吓尿了。等着，劳资要把这些流弊全清理了，擦擦。


<br />
## 行文花絮
覆盖通用采集；提供收集器，约定数据准入规范

预定义，大大增加用户push数据的成本。因此，不必预定义监控指标。

监控实体，用hostgrp管理，减少因为实体数量而带来的配置复杂度、实现动态生效。

监控策略，用template管理，减少报警配置的数量；结合继承、覆盖，减少template配置的复杂度。使用tag做策略过滤，提高灵活性、减少配置数据量。

